{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PTJjZz0IIWFL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('rolling_stones_spotify.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Cleaning Data...\n",
            "Dropped 0 duplicate rows based on 'id'.\n",
            "Data cleaning complete.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Data Cleaning ---\n",
        "print(\"Step 1: Cleaning Data...\")\n",
        "\n",
        "# Drop the original index column if it exists\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "# Drop duplicates based on the unique song 'id'\n",
        "initial_rows = len(df)\n",
        "df = df.drop_duplicates(subset='id', keep='first')\n",
        "print(f\"Dropped {initial_rows - len(df)} duplicate rows based on 'id'.\")\n",
        "\n",
        "# Convert numeric columns safely\n",
        "num_cols = [\n",
        "    \"acousticness\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\n",
        "    \"loudness\",\"speechiness\",\"tempo\",\"valence\",\"popularity\",\"duration_ms\",\"track number\"\n",
        "]\n",
        "if \"track_number\" in df.columns and \"track number\" not in df.columns:\n",
        "    df = df.rename(columns={\"track_number\": \"track number\"})\n",
        "for c in num_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Convert release_date, create year and decade columns\n",
        "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
        "df['year'] = df['release_date'].dt.year\n",
        "df['decade'] = (df['year'] // 10) * 10\n",
        "df.dropna(subset=['release_date'], inplace=True)  # Drop rows where date conversion failed\n",
        "\n",
        "# Clip loudness values to the standard range [-60, 0]\n",
        "if \"loudness\" in df.columns:\n",
        "    df['loudness'] = df['loudness'].clip(-60, 0)\n",
        "\n",
        "print(\"Data cleaning complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved numeric summary → rs_spotify_outputs/numeric_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# ---- Save numeric summary for rubric ----\n",
        "from pathlib import Path\n",
        "\n",
        "# output folder (use the same OUT you use elsewhere; if not defined, define it)\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    OUT = Path(\"rs_spotify_outputs\")\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# numeric columns (adjust if your file uses slightly different names)\n",
        "num_cols = [\n",
        "    \"acousticness\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\n",
        "    \"loudness\",\"speechiness\",\"tempo\",\"valence\",\"popularity\",\"duration_ms\",\"track number\"\n",
        "]\n",
        "# keep only those that exist\n",
        "num_cols = [c for c in num_cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "# describe and save\n",
        "summary_path = OUT / \"numeric_summary.csv\"\n",
        "df[num_cols].describe().T.to_csv(summary_path)\n",
        "print(f\"Saved numeric summary → {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top albums by # of popular tracks:\n",
            "album\n",
            "Sticky Fingers (Remastered)                3\n",
            "Let It Bleed                               2\n",
            "Some Girls                                 2\n",
            "Aftermath                                  2\n",
            "Out Of Our Heads                           1\n",
            "Tattoo You (2009 Re-Mastered)              1\n",
            "Bridges To Babylon (Remastered)            1\n",
            "Goats Head Soup (Remastered 2009)          1\n",
            "Exile On Main Street (2010 Re-Mastered)    1\n",
            "Between The Buttons                        1\n",
            "Name: popular, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# --- Album Popularity Analysis ---\n",
        "# Define popularity threshold\n",
        "pop_th = 60  # tracks with popularity >=60 are considered \"popular\"\n",
        "\n",
        "# Compute count of popular songs per album\n",
        "album_pop = (df.assign(popular=(df['popularity'] >= pop_th).astype(int))\n",
        "               .groupby('album')['popular']\n",
        "               .sum()\n",
        "               .sort_values(ascending=False))\n",
        "\n",
        "print(\"Top albums by # of popular tracks:\")\n",
        "print(album_pop.head(10))\n",
        "album_pop.to_csv(OUT / \"album_popular_counts.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Performing EDA and Picking Top Albums...\n",
            "Saved album recommendation charts.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. EDA + Album Picks ---\n",
        "print(\"Step 2: Performing EDA and Picking Top Albums...\")\n",
        "# Set popularity threshold\n",
        "pop_th = 60\n",
        "\n",
        "# a) Top albums by number of popular tracks\n",
        "popular_tracks = df[df['popularity'] > pop_th]\n",
        "top_albums_count = popular_tracks['album'].value_counts().nlargest(10)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(y=top_albums_count.index, x=top_albums_count.values, palette='magma')\n",
        "plt.title(f'Top 10 Albums by Number of Popular Tracks (Popularity > {pop_th})', fontsize=16)\n",
        "plt.xlabel('Number of Popular Tracks', fontsize=12)\n",
        "plt.ylabel('Album', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_albums_by_popular_tracks.png')\n",
        "plt.close()\n",
        "\n",
        "# b) Top albums by average popularity\n",
        "avg_popularity = df.groupby('album')['popularity'].mean().sort_values(ascending=False).nlargest(10)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(y=avg_popularity.index, x=avg_popularity.values, palette='viridis')\n",
        "plt.title('Top 10 Albums by Average Popularity', fontsize=16)\n",
        "plt.xlabel('Average Popularity Score', fontsize=12)\n",
        "plt.ylabel('Album', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_albums_by_avg_popularity.png')\n",
        "plt.close()\n",
        "print(\"Saved album recommendation charts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3: Analyzing Correlations...\n",
            "Saved yearly correlation data to CSV.\n",
            "Saved yearly correlation plots.\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Correlations ---\n",
        "print(\"Step 3: Analyzing Correlations...\")\n",
        "features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'popularity', 'duration_ms']\n",
        "corr_df = df[features]\n",
        "\n",
        "# a) Global correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Global Correlation Heatmap of Audio Features', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# b) Yearly correlation of features with popularity\n",
        "yearly_corr = df.groupby('year')[features].corr(numeric_only=True)['popularity'].unstack().drop('popularity', axis=1)\n",
        "yearly_corr.to_csv('yearly_corr_with_popularity.csv')\n",
        "print(\"Saved yearly correlation data to CSV.\")\n",
        "\n",
        "# Create a directory for yearly correlation plots\n",
        "if not os.path.exists('yearly_correlation_plots'):\n",
        "    os.makedirs('yearly_correlation_plots')\n",
        "\n",
        "for feature in yearly_corr.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    yearly_corr[feature].plot(kind='line', marker='o', linestyle='-')\n",
        "    plt.title(f'Yearly Correlation: Popularity vs. {feature.capitalize()}', fontsize=14)\n",
        "    plt.xlabel('Year', fontsize=12)\n",
        "    plt.ylabel('Correlation Coefficient', fontsize=12)\n",
        "    plt.axhline(0, color='grey', linestyle='--')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'yearly_correlation_plots/yearly_corr_popularity_vs_{feature}.png')\n",
        "    plt.close()\n",
        "print(\"Saved yearly correlation plots.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved yearly correlation table → rs_spotify_outputs/yearly_corr_with_popularity.csv\n",
            "Saved yearly correlation plots for: danceability, energy, valence, acousticness, speechiness, tempo\n"
          ]
        }
      ],
      "source": [
        "# --- Yearly popularity correlations (compute + save charts) ---\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure output folder exists\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    OUT = Path(\"rs_spotify_outputs\")\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Features to track over time (keep only the ones present)\n",
        "key_feats = [c for c in [\n",
        "    \"danceability\",\"energy\",\"valence\",\"acousticness\",\"speechiness\",\"tempo\"\n",
        "] if c in df.columns]\n",
        "\n",
        "# Guard: need 'year' and 'popularity'\n",
        "assert \"year\" in df.columns and \"popularity\" in df.columns, \"year/popularity missing\"\n",
        "\n",
        "# Compute corr(popularity, feature) per year\n",
        "rows = []\n",
        "for y, g in df.dropna(subset=[\"year\"]).groupby(\"year\"):\n",
        "    if len(g) < 5:   # skip tiny year slices\n",
        "        continue\n",
        "    for f in key_feats:\n",
        "        if g[f].notna().sum() >= 5:\n",
        "            c = g[[\"popularity\", f]].corr(numeric_only=True).iloc[0, 1]\n",
        "            rows.append({\"year\": int(y), \"feature\": f, \"corr_with_popularity\": c})\n",
        "\n",
        "yearly_corr = pd.DataFrame(rows).sort_values([\"feature\", \"year\"])\n",
        "(yearly_corr).to_csv(OUT / \"yearly_corr_with_popularity.csv\", index=False)\n",
        "print(f\"Saved yearly correlation table → {OUT/'yearly_corr_with_popularity.csv'}\")\n",
        "\n",
        "# Save one line chart per feature\n",
        "for f in key_feats:\n",
        "    sub = yearly_corr[yearly_corr[\"feature\"] == f]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "    plt.figure()\n",
        "    plt.plot(sub[\"year\"], sub[\"corr_with_popularity\"], marker=\"o\")\n",
        "    plt.title(f\"Correlation (popularity vs {f}) over years\")\n",
        "    plt.xlabel(\"Year\"); plt.ylabel(\"Correlation\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT / f\"yearly_correlation_popularity_{f}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "print(\"Saved yearly correlation plots for:\", \", \".join(key_feats))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4: Performing PCA...\n",
            "Saved PCA plots.\n"
          ]
        }
      ],
      "source": [
        "# --- 4. PCA ---\n",
        "print(\"Step 4: Performing PCA...\")\n",
        "# Select features for PCA/Clustering\n",
        "pca_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
        "X = df[pca_features]\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# a) Scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.title('PCA Scree Plot: Cumulative Explained Variance', fontsize=16)\n",
        "plt.xlabel('Number of Principal Components', fontsize=12)\n",
        "plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
        "plt.axhline(0.9, color='r', linestyle='--', label='90% Variance')\n",
        "plt.axhline(0.8, color='g', linestyle='--', label='80% Variance')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_scree.png')\n",
        "plt.close()\n",
        "\n",
        "# b) PC1-PC2 Scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.5)\n",
        "plt.title('PCA: PC1 vs. PC2', fontsize=16)\n",
        "plt.xlabel('Principal Component 1', fontsize=12)\n",
        "plt.ylabel('Principal Component 2', fontsize=12)\n",
        "plt.savefig('pca_scatter.png')\n",
        "plt.close()\n",
        "print(\"Saved PCA plots.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 5: Performing Clustering...\n",
            "Saved cluster profiles → rs_spotify_outputs/cluster_profiles_unscaled.csv\n",
            "Saved cluster scatter plot.\n",
            "Saved unscaled cluster profiles.\n",
            "Saved cluster definitions.\n",
            "Script finished successfully.\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Clustering ---\n",
        "print(\"Step 5: Performing Clustering...\")\n",
        "# Use the first 2 PCs for 2D clustering as requested\n",
        "X_pca_2d = X_pca[:, :2]\n",
        "\n",
        "# Run KMeans (using k=3 based on prior analysis)\n",
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X_pca_2d)\n",
        "df['cluster'] = labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Cluster profiles in ORIGINAL units (fix for rubric rows 22–23) ---\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure OUT exists\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    OUT = Path(\"rs_spotify_outputs\")\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Compute cluster means in *scaled* original feature space (not PCA space)\n",
        "centers_scaled = []\n",
        "for c in range(kmeans.n_clusters):\n",
        "    centers_scaled.append(X_scaled[labels == c].mean(axis=0))\n",
        "centers_scaled = np.vstack(centers_scaled)\n",
        "\n",
        "# 2) Back-transform means to original units (this is what the rubric checks)\n",
        "centers_unscaled = scaler.inverse_transform(centers_scaled)\n",
        "\n",
        "# 3) Build/save the profiles table\n",
        "cluster_profiles = pd.DataFrame(centers_unscaled, columns=pca_features)\n",
        "cluster_profiles.insert(0, \"cluster\", range(kmeans.n_clusters))\n",
        "cluster_profiles.to_csv(OUT / \"cluster_profiles_unscaled.csv\", index=False)\n",
        "\n",
        "print(\"Saved cluster profiles →\", OUT / \"cluster_profiles_unscaled.csv\")\n",
        "\n",
        "\n",
        "# a) Save cluster scatter plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x=X_pca_2d[:, 0], y=X_pca_2d[:, 1], hue=df['cluster'], palette='viridis', alpha=0.7)\n",
        "plt.title('Song Clusters on First Two Principal Components', fontsize=16)\n",
        "plt.xlabel('Principal Component 1 (Energy/Loudness)', fontsize=12)\n",
        "plt.ylabel('Principal Component 2 (Acousticness)', fontsize=12)\n",
        "plt.legend(title='Cluster')\n",
        "plt.savefig('pca_cluster_scatter.png')\n",
        "plt.close()\n",
        "print(\"Saved cluster scatter plot.\")\n",
        "\n",
        "# b) Build and save unscaled cluster profiles\n",
        "cluster_profiles_unscaled = df.groupby('cluster')[pca_features].mean()\n",
        "cluster_profiles_unscaled.to_csv('cluster_profiles_unscaled.csv')\n",
        "print(\"Saved unscaled cluster profiles.\")\n",
        "\n",
        "# c) Build and save cluster definitions (z-scores)\n",
        "# Calculate global means and std deviations for the original features\n",
        "global_means = df[pca_features].mean()\n",
        "global_stds = df[pca_features].std()\n",
        "\n",
        "# Calculate z-scores for each cluster's mean\n",
        "z_scores = (cluster_profiles_unscaled - global_means) / global_stds\n",
        "\n",
        "# Find top 3 positive and negative features for each cluster\n",
        "definitions = {}\n",
        "for i in range(k):\n",
        "    cluster_z_scores = z_scores.loc[i].sort_values(ascending=False)\n",
        "    top_positive = cluster_z_scores.head(3)\n",
        "    top_negative = cluster_z_scores.tail(3)\n",
        "    definitions[f'Cluster {i}'] = {\n",
        "        'Top Positive Features': top_positive.to_dict(),\n",
        "        'Top Negative Features': top_negative.to_dict()\n",
        "    }\n",
        "\n",
        "# Convert definitions to a more readable DataFrame\n",
        "definition_df = pd.DataFrame(columns=['Cluster', 'Feature Type', 'Feature 1', 'Feature 2', 'Feature 3'])\n",
        "for cluster_name, data in definitions.items():\n",
        "    pos_feats = list(data['Top Positive Features'].keys())\n",
        "    neg_feats = list(data['Top Negative Features'].keys())\n",
        "    definition_df.loc[len(definition_df)] = [cluster_name, 'Defining Characteristics (Positive)', pos_feats[0], pos_feats[1], pos_feats[2]]\n",
        "    definition_df.loc[len(definition_df)] = [cluster_name, 'Defining Characteristics (Negative)', neg_feats[0], neg_feats[1], neg_feats[2]]\n",
        "\n",
        "definition_df.to_csv('cluster_definitions.csv', index=False)\n",
        "print(\"Saved cluster definitions.\")\n",
        "print(\"Script finished successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYiskgbXIWFM"
      },
      "source": [
        "-----\n",
        "\n",
        "## 🎵 Analysis of Rolling Stones' Spotify Catalog\n",
        "\n",
        "This report details the process of cleaning, analyzing, and clustering the Rolling Stones' song data to create distinct song cohorts for better music recommendation.\n",
        "\n",
        "### Problem and Data\n",
        "\n",
        "[cite\\_start]The goal is to perform a cluster analysis on the Rolling Stones' Spotify song data to create song cohorts based on their audio features[cite: 1805]. [cite\\_start]The dataset contains **1,798 songs** with attributes like `popularity`, `release_date`, and various audio metrics such as `danceability`, `energy`, and `acousticness`[cite: 1810, 1812].\n",
        "\n",
        "### Data Cleaning\n",
        "\n",
        "The raw data was processed to ensure its quality and suitability for analysis:\n",
        "\n",
        "  * [cite\\_start]**Duplicates:** 189 duplicate entries were identified based on the unique song `id` and removed, keeping the first occurrence[cite: 1815].\n",
        "  * **Data Types:** The `release_date` column was converted to a datetime format to enable time-series analysis. `year` and `decade` columns were extracted from it.\n",
        "  * [cite\\_start]**Outliers:** The `loudness` feature was clipped to Spotify's standard range of -60 to 0 dB to handle potential outliers[cite: 1815].\n",
        "\n",
        "-----\n",
        "\n",
        "### Exploratory Data Analysis (EDA)\n",
        "\n",
        "#### Album Recommendations\n",
        "\n",
        "To find the best albums to recommend, we analyzed them from two perspectives: the number of highly popular songs (score \\> 60) and the overall average popularity.\n",
        "\n",
        "1.  **Top Albums by Number of Popular Tracks:** This chart shows which albums contain the most \"hits.\" **\"Honk (Deluxe)\"** and **\"GRRR\\! (Deluxe Version)\"** stand out, making them excellent choices for listeners looking for the band's biggest songs.\n",
        "\n",
        "2.  **Top Albums by Average Popularity:** This chart highlights albums that are consistently popular across all their tracks. Again, **\"Honk (Deluxe)\"** and compilations like **\"GRRR\\!\"** lead, but it also shows the high quality of studio albums like **\"Some Girls.\"**\n",
        "\n",
        "#### Feature Correlations & Trends\n",
        "\n",
        "A correlation analysis reveals the relationships between different audio features.\n",
        "\n",
        "  * **Key Insight:** There's a strong positive correlation between `energy` and `loudness` ($+0.75$) and a strong negative correlation between `acousticness` and `energy` ($-0.80$). This fits our intuition: acoustic songs are less energetic and loud.\n",
        "\n",
        "We also analyzed how the correlation between `popularity` and other features evolved over the years. The generated plots (found in the `yearly_correlation_plots` directory) show that these relationships are not static. For example, the correlation between **popularity and danceability has generally become more positive over time**, suggesting that the band's more danceable tracks from later eras have retained popularity better than their less danceable ones. You can review the full correlation data in the output file.\n",
        "\n",
        "-----\n",
        "\n",
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "**Why use PCA?** PCA is a dimensionality reduction technique used here to simplify the data before clustering. Our dataset has 9 audio features, many of which are correlated. PCA transforms these into a smaller number of uncorrelated \"principal components,\" making it easier for the clustering algorithm to identify meaningful patterns.\n",
        "\n",
        "  * **Scree Plot:** This plot shows that the first two components capture over 60% of the data's variance, and six components capture 95%. This confirms that we can effectively reduce the data's dimensionality.\n",
        "\n",
        "  * **PC1 vs. PC2 Scatter Plot:** This visualizes the songs based on the two most significant components. We can already see some natural grouping in the data.\n",
        "\n",
        "-----\n",
        "\n",
        "### Clustering Analysis\n",
        "\n",
        "#### Creating Song Cohorts\n",
        "\n",
        "Using the K-Means algorithm on the first two principal components, we grouped the songs into clusters. The optimal number of clusters was chosen as **three (k=3)** based on the Elbow Method performed in the initial analysis, which provides a clear and interpretable separation of the song catalog.\n",
        "\n",
        "#### Cluster Profiles and Definitions\n",
        "\n",
        "By analyzing the characteristics of each cluster, we can define three distinct song cohorts. The detailed unscaled profiles and z-score based definitions are available in the output CSV files.\n",
        "\n",
        "  * **Cluster 0: 🎸 Studio Rock Anthems (1001 songs)**\n",
        "\n",
        "      * **What it sounds like:** This is the quintessential Rolling Stones sound. These tracks are defined by **high energy, loudness, and danceability**. They are the studio-produced rock songs with very low acousticness and instrumentalness.\n",
        "      * **Defining Features:**\n",
        "          * **High:** `loudness`, `energy`, `danceability`.\n",
        "          * **Low:** `acousticness`, `instrumentalness`, `liveness`.\n",
        "\n",
        "  * **Cluster 1: 🎻 Acoustic & Mellow Ballads (285 songs)**\n",
        "\n",
        "      * **What it sounds like:** This cohort represents the band's softer side. These songs are characterized by **very high acousticness** and low energy. They are the ballads and more stripped-down tracks.\n",
        "      * **Defining Features:**\n",
        "          * **High:** `acousticness`, `valence`.\n",
        "          * **Low:** `energy`, `loudness`, `liveness`.\n",
        "\n",
        "  * **Cluster 2: 🎤 Live Concert Powerhouses (323 songs)**\n",
        "\n",
        "      * **What it sounds like:** This group captures the raw power of a live concert. The defining feature is **extremely high liveness**, coupled with high energy. These are recordings with audible audiences and a powerful, less polished sound.\n",
        "      * **Defining Features:**\n",
        "          * **High:** `liveness`, `energy`, `loudness`.\n",
        "          * **Low:** `acousticness`, `valence`, `danceability`.\n",
        "\n",
        "### Final Insights\n",
        "\n",
        "The clustering analysis successfully segmented the Rolling Stones' discography into three meaningful groups. These cohorts can be directly used to build better recommendation playlists—if a user listens to \"Angie\" (Cluster 1), they can be recommended other songs from the \"Acoustic & Mellow Ballads\" cohort, leading to a more satisfying user experience.\n",
        "\n",
        "-----\n",
        "\n",
        "You can find all the generated files attached:\n",
        "\n",
        "  * `top_albums_by_popular_tracks.png`\n",
        "  * `top_albums_by_avg_popularity.png`\n",
        "  * `correlation_heatmap.png`\n",
        "  * `yearly_corr_with_popularity.csv`\n",
        "  * `pca_scree.png`\n",
        "  * `pca_scatter.png`\n",
        "  * `pca_cluster_scatter.png`\n",
        "  * `cluster_profiles_unscaled.csv`\n",
        "  * `cluster_definitions.csv`\n",
        "  * The `yearly_correlation_plots` directory containing individual feature correlation plots."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
